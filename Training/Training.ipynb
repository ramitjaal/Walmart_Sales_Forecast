{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "from sklearn.model_selection import GroupKFold\n",
    "# custom imports\n",
    "from multiprocessing import Pool        \n",
    "from Custom_Metric import custom_metric\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    \n",
    "## Multiprocess Runs\n",
    "def df_parallelize_run(func, t_split):\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helper to load data by store ID\n",
    "#################################################################################\n",
    "\n",
    "## Features to remove\n",
    "## State based features only\n",
    "def removeFeatures(store):\n",
    "    if 'CA' in store:\n",
    "        remove_features = ['id','state_id','store_id','date','tm_w_end','nwd_TX','nwd_WI',\n",
    "                           'groups','wm_yr_wk','d',TARGET]\n",
    "    elif 'TX' in store:\n",
    "        remove_features = ['id','state_id','store_id','date','tm_w_end','nwd_CA','nwd_WI',\n",
    "                           'groups','wm_yr_wk','d',TARGET]\n",
    "    else:\n",
    "        remove_features = ['id','state_id','store_id','date','tm_w_end','nwd_CA','nwd_TX',\n",
    "                           'groups','wm_yr_wk','d',TARGET]\n",
    "    return remove_features\n",
    "\n",
    "# MEAN FEATURES\n",
    "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
    "                   'enc_dept_id_mean','enc_dept_id_std',\n",
    "                   'enc_item_id_mean','enc_item_id_std'] \n",
    "\n",
    "rolls = ['rolling_mean_tmp_1_7','rolling_mean_tmp_1_14','rolling_mean_tmp_1_30',\n",
    "            'rolling_mean_tmp_7_7','rolling_mean_tmp_7_14','rolling_mean_tmp_7_30',\n",
    "            'rolling_mean_tmp_14_7','rolling_mean_tmp_14_14','rolling_mean_tmp_14_30']\n",
    "# Read data\n",
    "def get_data_by_store(store):\n",
    "    \n",
    "    # Read and contact basic feature\n",
    "    df = pd.concat([pd.read_pickle(BASE),\n",
    "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
    "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "                    axis=1)\n",
    "    \n",
    "    # Only relevant store\n",
    "    df = df[df['store_id']==store]\n",
    "\n",
    "    # Reading seperately so that memory limit is not reached\n",
    "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2 # to not reach memory limit \n",
    "    \n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3 # to not reach memory limit \n",
    "    \n",
    "    remove_features = removeFeatures(store)\n",
    "    # Create features list\n",
    "    features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    # Skipping first n rows\n",
    "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "# Recombine Test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in STORES_IDS:\n",
    "        temp_df = pd.read_pickle('/My Drive/Walmart_Data/models/test_'+store_id+'.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test\n",
    "\n",
    "\n",
    "########################### dynamic rolling lags\n",
    "#################################################################################\n",
    "def make_lag(LAG_DAY):\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "\n",
    "def make_lag_roll(LAG_DAY):\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).ewm(span=roll_wind).mean())\n",
    "    return lag_df[[col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "#################################################################################\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'custom',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': 1,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "SEED = 42                        \n",
    "seed_everything(SEED)            \n",
    "lgb_params['seed'] = SEED        \n",
    "N_CORES = psutil.cpu_count()     \n",
    "\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1941               # End day of our train set\n",
    "P_HORIZON   = 56                 # Prediction horizon\n",
    "USE_AUX     = True               # Pretrained models\n",
    "\n",
    "#PATHS for Features\n",
    "ORIGINAL = '/My Drive/Walmart_Data/train/'\n",
    "BASE     = '/My Drive/Walmart_Data/grid_part_1.pkl'\n",
    "PRICE    = '/My Drive/Walmart_Data/grid_part_2.pkl'\n",
    "CALENDAR = '/My Drive/Walmart_Data/grid_part_3.pkl'\n",
    "LAGS     = '/My Drive/Walmart_Data/lags_df_28.pkl'\n",
    "MEAN_ENC = '/My Drive/Walmart_Data/mean_encoding_df.pkl'\n",
    "\n",
    "# AUX(pretrained) Models paths\n",
    "AUX_MODELS = '/My Drive/Walmart_Data/models/'\n",
    "CV_FOLDS   = [0,1,2]\n",
    "\n",
    "#STORES ids\n",
    "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())\n",
    "\n",
    "#SPLITS for lags creation\n",
    "SHIFT_DAY  = 28\n",
    "N_LAGS     = 15\n",
    "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
    "ROLS_SPLIT = []\n",
    "for i in [1,7,14]:\n",
    "    for j in [7,14,30,60]:\n",
    "        ROLS_SPLIT.append([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store in STORES_IDS:\n",
    "    print('Train', store)       \n",
    "    grid, features_columns = get_data_by_store(store)\n",
    "\n",
    "    train_mask = grid_df['d']<=END_TRAIN\n",
    "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
    "\n",
    "    ## We will use oof kfold to find \"best round\"\n",
    "    folds = GroupKFold(n_splits=3)\n",
    "\n",
    "    # get subgroups for each week, year pair\n",
    "    grid_df['groups'] = grid_df['tm_w'].astype(str) + '_' + grid_df['tm_y'].astype(str)\n",
    "    split_groups = grid_df[train_mask]['groups']\n",
    "\n",
    "    # Main Data\n",
    "    X,y = grid_df[train_mask][features_columns], grid_df[train_mask][TARGET]\n",
    "\n",
    "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "    grid_df = grid_df[keep_cols]\n",
    "    grid_df.to_pickle('/My Drive/Walmart_Data/models/test_'+store+'.pkl')\n",
    "    del grid_df\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=split_groups)):\n",
    "        print('Fold:',fold_)\n",
    "        print(len(trn_idx),len(val_idx))\n",
    "        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n",
    "        v_X, v_y   = X.iloc[val_idx,:], y[val_idx] \n",
    "        train_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        valid_data = lgb.Dataset(v_X, label=v_y)  \n",
    "\n",
    "        estimator = lgb.train(\n",
    "              lgb_params,\n",
    "              train_data,\n",
    "              valid_sets = [train_data, valid_data],\n",
    "              verbose_eval = 100,\n",
    "              metric=custom_metric\n",
    "          )\n",
    "\n",
    "        model_name = '/My Drive/Walmart_Data/models/lgb_model_'+store+'_'+str(fold_)+'.bin'\n",
    "        pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "        # Remove temporary files and objects\n",
    "        del train_data, valid_data, estimator\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Predict\n",
    "#################################################################################\n",
    "for fold_ in CV_FOLDS:\n",
    "    print(\"FOLD:\", fold_)\n",
    "    all_preds = pd.DataFrame()\n",
    "    base_test = get_base_test()\n",
    "    modelFeatures=base_test.columns\n",
    "    \n",
    "    # Timer to measure predictions time \n",
    "    main_time = time.time()\n",
    "\n",
    "    # Loop over each prediction day\n",
    "    # As rolling lags are the most timeconsuming\n",
    "    # we will calculate it for whole day\n",
    "    for PREDICT_DAY in range(1,29):    \n",
    "        print('Predict | Day:', PREDICT_DAY)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Make temporary grid to calculate rolling lags\n",
    "        grid_df = base_test.copy()\n",
    "        grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
    "        MODEL_FEATURES=[col for col in modelFeatures if col not in removeFeatures(STORES_IDS[0])] + rolls\n",
    "\n",
    "        for store_id in STORES_IDS:\n",
    "\n",
    "            model_path = '/My Drive/Walmart_Data/models/lgb_model_'+store_id+'_'+str(fold_)+'.bin' \n",
    "            \n",
    "            if USE_AUX:\n",
    "                model_path = AUX_MODELS + model_path\n",
    "\n",
    "            estimator = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "            day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
    "            store_mask = base_test['store_id']==store_id\n",
    "\n",
    "            mask = (day_mask)&(store_mask)\n",
    "            base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
    "\n",
    "\n",
    "        temp_df = base_test[day_mask][['id',TARGET]]\n",
    "        temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
    "        if 'id' in list(all_preds):\n",
    "            all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
    "        else:\n",
    "            all_preds = temp_df.copy()\n",
    "        all_preds = all_preds.reset_index(drop=True)\n",
    "        print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
    "                      ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
    "                      ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
    "    all_preds.to_csv('/My Drive/Walmart_Data/models/all_preds_CA_'+str(fold_)+'.csv',index=False)\n",
    "    del temp_df, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Predictions\n",
    "all_preds_0=pd.read_csv(AUX_MODELS+'all_preds_CA_0'+'.csv')\n",
    "all_preds_1=pd.read_csv(AUX_MODELS+'all_preds_CA_1'+'.csv')\n",
    "all_preds_2=pd.read_csv(AUX_MODELS+'all_preds_CA_2'+'.csv')\n",
    "\n",
    "# Create Dummy DataFrame to store predictions\n",
    "final_all_preds = pd.DataFrame()\n",
    "final_all_preds['id'] = all_preds_1['id']\n",
    "for item in all_preds_1:\n",
    "    if item!='id':\n",
    "        final_all_preds[item]=(all_preds_0[item]*(1/3))+(all_preds_1[item]*(1/3))+(all_preds_2[item]*(1/3))\n",
    "final_all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Export\n",
    "#################################################################################\n",
    "\n",
    "submission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\n",
    "submission = submission.merge(final_all_preds, on=['id'], how='inner').fillna(0)\n",
    "submission.to_csv(AUX_MODELS+'submission_CA'+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
